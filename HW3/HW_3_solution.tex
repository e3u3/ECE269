%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% {
\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cases}


\hypersetup{%
colorlinks=true,
linkcolor=blue,
linkbordercolor={0 0 1}
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }

%%%%%%%%%%%%%%%%%%%%%%%% CHANGE HERE %%%%%%%%%%%%%%%%%%%% {
\newcommand\course{ECE 269}
\newcommand\semester{Fall 2019}
\newcommand\hwnumber{\#3}                 % <-- ASSIGNMENT #
\newcommand\NetIDa{Jiaming Lai}           % <-- YOUR NAME
\newcommand\NetIDb{A53314574}           % <-- STUDENT ID #
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }

%%%%%%%%%%%%%%%%% DO NOT CHANGE HERE %%%%%%%%%%%%%%%%%%%% {
\pagestyle{fancyplain}
\headheight 35pt
\lhead{\NetIDa}
\lhead{\NetIDa\\\NetIDb}                 
\chead{\textbf{\Large Assignment \hwnumber}}
\rhead{\course \\ \semester}
\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% }

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 1: Orthogonal Complement of a Subspace}

\textbf{Solution}

\begin{enumerate}[(a)]
    %%%%%%%%%%%
    % Item (a)
    %%%%%%%%%%%
    \item 
    Suppose $x_1, x_2 \in \mathcal{V}^\perp$, for any $y\in \mathcal{V}$
    \begin{equation}
        (x_1+x_2)^Ty = (x_1^T+x_2^T)y = x_1^Ty+x_2^Ty = 0 \nonumber
    \end{equation}
    For any $\alpha \in \mathbb{R}$,
    \begin{equation}
        (\alpha x_1)^Ty = \alpha x_1^Ty = 0 \nonumber
    \end{equation}
    Hence $\mathcal{V}^\perp$ is a subspace of $\mathbb{R}^n$.
    %%%%%%%%%%%
    % Item (b)
    %%%%%%%%%%%
    \item 
    Because $\mathcal{V}=span(v_1,v_2,\ldots,v_k)$, for any $y\in \mathcal{V}$,
    \begin{equation}
        y = \sum_{i=1}^{k}\alpha_i v_i =
        \begin{bmatrix}
            v_1 & v_2 & \ldots & v_k
        \end{bmatrix}
        \begin{bmatrix}
            \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_k
        \end{bmatrix},
        \ where\ \alpha_1, \ldots ,\alpha_k \in \mathbb{R}
        \nonumber
    \end{equation}
    Hence $\mathcal{V} = R(A)$.\\
    For any $y\in \mathcal{V}$,
    \begin{equation}
        x^Ty = x^TA
        \begin{bmatrix}
            \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_k
        \end{bmatrix}
        =0,\ \ where\ \alpha_1, \ldots ,\alpha_k \in \mathbb{R}
        \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow \left(A \begin{bmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_k \end{bmatrix}\right)^Tx
        =0
        \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow \begin{bmatrix} \alpha_1 & \alpha_2 & \ldots & \alpha_k \end{bmatrix}A^Tx
        =0
        \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow A^Tx=0
        \nonumber
    \end{equation}
    Hence $\mathcal{V^\perp} = N(A^T)$.
    %%%%%%%%%%%
    % Item (c)
    %%%%%%%%%%%
    \item 
    $(\mathcal{V}^\perp)^\perp$ can be represented as the following:
    \begin{equation}
        (\mathcal{V}^\perp)^\perp=\{y\in \mathbb{R}^n:y^Tx=0,\forall x \in \mathcal{V}^\perp\}
        \nonumber
    \end{equation}
    Meanwhile, for any $x\in \mathcal{V}^\perp$,
    \begin{equation}
        x^Ty=0,\ \forall y \in \mathcal{V} \Rightarrow y^Tx=0,\ \forall y \in \mathcal{V}
        \nonumber
    \end{equation}
    Hence $(\mathcal{V}^\perp)^\perp=\mathcal{V}$
    %%%%%%%%%%%
    % Item (d)
    %%%%%%%%%%%
    \item 
    \begin{equation}
        dim(\mathcal{V})=dim[R(A)]=rank(A)=rank(A^T)=dim[R(A^T)] \nonumber
    \end{equation}
    Meanwhile
    \begin{equation}
        dim(\mathcal{V}^\perp)=dim[N(A^T)] \nonumber
    \end{equation}
    Hence
    \begin{equation}
        dim(\mathcal{V})+dim(\mathcal{V}^\perp)=dim[R(A^T)]+dim[N(A^T)]=n \nonumber
    \end{equation}
    %%%%%%%%%%%
    % Item (e)
    %%%%%%%%%%%
    \item 
    Accoding to the definition,
    \begin{equation}
        \mathcal{W}^\perp = \left\{x_2\in \mathbb{R}^n:\ x_2^Ty_2=0,\ \forall y_2\in \mathcal{W}\right\} \nonumber
    \end{equation}
    For any $y_1\in \mathcal{V}$. Because $\mathcal{V} \subseteq \mathcal{W}$, $y_1\in \mathcal{W}$, hence
    \begin{equation}
        x_2^Ty_1=0\ \Rightarrow\ x_2\in \mathcal{V}^\perp \nonumber
    \end{equation}
    Hence $\mathcal{V} \subseteq \mathcal{W}$ for another subspace $\mathcal{W}$ implies $\mathcal{W}^\perp \subseteq \mathcal{V}^\perp$
    %%%%%%%%%%%
    % Item (f)
    %%%%%%%%%%%
    \item 
    Suppose $v$ is the projection of $x$ onto subspace $\mathcal{V}$. Then
    \begin{equation}
        <x-v,y>=0,\ \forall y\in \mathcal{V} \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow y^T(x-v)=0,\ \forall y\in \mathcal{V} \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow (x-v)^Ty=0,\ \forall y\in \mathcal{V} \nonumber
    \end{equation}
    So $(x-v) \in \mathcal{V}^\perp$. Suppose there is a vector $v^\perp$, S.T.
    \begin{equation}
        x-v=v^\perp \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow x=v+v^\perp \nonumber
    \end{equation}
    Meanwhile, because $v$ is the projection of $x$ onto subspace $\mathcal{V}$, $v$ must be unique and thus
    $v^\perp$ is also unique. Hence any $x\in \mathbb{R}^n$ could be expressed uniquely as $x=v+v^\perp$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 2: Rank of a Product}

\textbf{Solution}
Suppose matrix A is
\begin{equation}
    \begin{bmatrix}
        a_1^T \\ a_2^T \\ a_3^T \\ a_4^T
    \end{bmatrix} \nonumber
\end{equation}
\begin{equation}
    \Rightarrow AB = \begin{bmatrix}B^Ta_1 & B^Ta_2 & B^Ta_3 & B^Ta_4\end{bmatrix}  \nonumber
\end{equation}
Because $Rank(A)=2$, there should be $\alpha_1,\alpha_2 \in \mathbb{R}$ such that
$a_3=\alpha_1a_1+\alpha_2a_2$. Hence $B^Ta_3=\alpha_1B^Ta_1+\alpha_2B^Ta_2$. The same
could be show that $B^Ta_4=\beta_1B^Ta_1+\beta_2B^Ta_2$. Hence there is two linear independent
vectors at most in ${a_1,a_2,a_3,a_4}$. Hence
\begin{equation}
    rank(AB) \leq 2 \nonumber
\end{equation}
The folloing will show that $B^Ta_1$ and $B^Ta_2$ is linear independent.
Suppose $\alpha_1,\alpha_2 \in \mathbb{R}$. $a_1$ and $a_2$ are linear independent.
\begin{equation}
    \alpha_1B^Ta_1+\alpha_2B^Ta_2=0 \Rightarrow B^T(\alpha_1a_1+\alpha_2a_2)=0 \nonumber
\end{equation}
Because $B$ is full-rank, so $dim(N(B^T))=3-3=0$. Hence $N(B^T)={0}$ and $\alpha_1a_1+\alpha_2a_2=0$.
Because $a_1$ and $a_2$ are linear independent, $\alpha_1=\alpha_2=0$. So $B^Ta_1$ and $B^Ta_2$
are linear independent. Hence
\begin{equation}
    rank(AB)=rank(B^TA^T) \ge 2 \nonumber
\end{equation}
Hence $r_{min}=r_{max}=2$. The following is the special case:
\begin{equation}
    A=\begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & 0 \\
    \end{bmatrix} \nonumber
\end{equation}
\begin{equation}
    B=\begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix} \nonumber
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 3: An Inequality for Orthonormal Matrices}

\textbf{Solution}
\begin{equation}
    ||U^Tx||_2 \leq ||x||_2 \Leftrightarrow ||U^Tx||^2_2 \leq ||x||^2_2
    \Leftrightarrow (U^Tx)^TU^Tx \leq x^Tx \nonumber
\end{equation}
Hence we only have to show that
\begin{equation}
    x^Tx-x^Tuu^Tx \ge 0 \nonumber
\end{equation}
\begin{equation}
    \Rightarrow x^T(I-uu^T)x \ge 0 \nonumber
\end{equation}
Suppose $v=(I-uu^T)$,
\begin{equation}
    \begin{split}
        v^Tv &=(I-uu^T)^T(I-uu^T) \\
        &= (I-uu^T)(I-uu^T) \\
        &= I-uu^T-uu^T+uu^Tuu^T \\
        &= I-uu^T-uu^T+uIu^T \\
        &= I-uu^T \\
        &= v
    \end{split} \nonumber
\end{equation}
Hence
\begin{equation}
    x^T(I-uu^T)x = x^Tvx = x^Tv^Tvx =<vx,vx> \nonumber
\end{equation}
Because $<vx,vx> \ge 0$, hence $x^T(I-uu^T)x \ge 0$. So $||U^Tx||_2 \leq ||x||_2$ is valid.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 4: Householder Reflections}

\textbf{Solution}
\begin{enumerate}[(a)]
    %%%%%%%%%%%
    % Item (a)
    %%%%%%%%%%%
    \item 
    \begin{equation}
        \begin{split}
            QQ^T &= (I-2uu^T)(I-2uu^T)^T \\
            &= (I-2uu^T)^T-2uu^T(I-2uu^T)^T \\
            &= I-2uu^T-2uu^T+4uu^Tuu^T \\
            &= I-4uu^T+4u(u^Tu)u^T \\
        \end{split} \nonumber
    \end{equation}
    Because $u$ is unit vector, $u^Tu = ||u||^2=1$. Hence $u(u^Tu)u^T=uu^T$, so $QQ^T=I$. Therefore,
    $Q$ is orthogonal.
    %%%%%%%%%%%
    % Item (b)
    %%%%%%%%%%%
    \item 
    \begin{equation}
        \begin{split}
            Qu &= (I-2uu^T)u \\
            &= Iu-2uu^Tu \\
            &= u-2u(u^Tu) \\
            &= u-2u \\
            &= -u
        \end{split} \nonumber
    \end{equation}
    \begin{equation}
        \begin{split}
            Qv &= (I-2uu^T)v \\
            &= v-2uu^Tv \\
            &= v-2u<v,u> \\
            &= v \\
        \end{split} \nonumber
    \end{equation}
    Suppose $\hat{x}$ is the orthogonal projection of x onto $span\{u\}$, and $x_1=x-\hat{x}$ and
    $x_2=\hat{x}=\alpha u$. In this case, $(x-\hat{x})\perp\hat{x} \Rightarrow x_1\perp x_2$ and $x=x_1+x_2$.
    Hence
    \begin{equation}
        Qx_2=Q\hat{x}=\alpha Qu=-\alpha u=-x_2 \nonumber
    \end{equation}
    \begin{equation}
        Qx_1=x_1 \nonumber
    \end{equation}
    Hence
    \begin{equation}
        y=Qx=Q(x_1+x_2)=x_1-x_2 \nonumber
    \end{equation}
    $\hat{x}$ is the orthogonal projection of x onto $span\{u\}$. $u$ is the normal vector of 
    hyperplane $u^Tx=0$ and so $x_1$ should be the projection of x onto hyperplane $u^Tx=0$.
    Because $x=x_1+x_2$ and $y=Qx=x_1-x_2$, orthogonal projection of x onto $span\{u\}$ changes to negative
    but the projection of x onto hyperplane $u^Tx=0$ stay unchanged. So $y=Qx$ is actually reflect $x$
    through the hyperplane with normal vector $u$.
    %%%%%%%%%%%
    % Item (c)
    %%%%%%%%%%%
    \item 
    Known $Q\in \mathbb{R}^{n\times n}$, suppose $u_1,u_2,\ldots,u_n$ is the column of $Q$.
    Because $Q$ is orthogonal, then $u_1,u_2,\ldots,u_n$ are orthogonal, hence $u_1,u_2,\ldots,u_n$
    are linearly independent. Thus matrix $Q$ is full-rank and inversible. Hence there is only one
    solution. Meanwhile
    \begin{equation}
        QQ=(I-2uu^T)(I-2uu^T)=Q \nonumber
    \end{equation}
    So
    \begin{equation}
        Qy=QQx=x \nonumber
    \end{equation}
    So the unique solution is $x=Qy$
    %%%%%%%%%%%
    % Item (d)
    %%%%%%%%%%%
    \item
    Obviously matirx $uu^T$ is square, hence
    \begin{equation}
        det(Q)=det(I-2uu^T)=det(I-2u^TuI)=det(I-2I)=det(-I)=-1 \nonumber
    \end{equation}
    %%%%%%%%%%%
    % Item (e)
    %%%%%%%%%%%
    \item
    Following problem (b). Suppose $\hat{x}$ is the orthogonal projection of x onto $span\{u\}$, and $x_1=x-\hat{x}$ and
    $x_2=\hat{x}=\alpha u$. Because $Qx\in y$, suppose $Qx=\beta y$. Hence we get
    \begin{numcases}{} \nonumber
        x=x_1+x_2  \\ \nonumber
        \beta y=x_1-x_2 
    \end{numcases} \nonumber
    so we get
    \begin{numcases}{} \nonumber
        x_1=0.5x+0.5\beta y  \\ \nonumber
        x_2=0.5x-0.5\beta y 
    \end{numcases} \nonumber
    Since $x_1\perp x_2$, so
    \begin{equation}
        <2x_1,2x_2>=<x+\beta y,x-\beta y>=||x||^2_2-\beta^2||y||^2_2=0 \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow \beta^2=\frac{||x||^2_2}{||y||^2_2} \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow x_2=0.5x-0.5\frac{||x||_2}{||y||_2}y \nonumber
    \end{equation}
    \begin{equation}
        \Rightarrow u=\frac{x-\frac{||x||_2}{||y||_2}y}{||x-\frac{||x||_2}{||y||_2}y||_2} \nonumber
    \end{equation}
    If $0.5x-0.5\frac{||x||_2}{||y||_2}y=0$, then any unit vector $u\perp x$ will make sense.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Problem 5: Projection Matrices}

\textbf{Solution}
\begin{enumerate}[(a)]
    %%%%%%%%%%%
    % Item (a)
    %%%%%%%%%%%
    \item 
    Obviously $I-P$ is also a symmetric matrix.
    \begin{equation}
        \begin{split}
            (I-P)(I-P) &= (I-P)-P(I-P)\\
            &= I-P-P+P^2 \\
            &= I-2P+P\\
            &= I-P \\
        \end{split} \nonumber
    \end{equation}
    So $I-P$ is also a projection matrix.
    %%%%%%%%%%%
    % Item (b)
    %%%%%%%%%%%
    \item
    Obviously $UU^T$ is symmetric matirx.
    \begin{equation}
        \begin{split}
            (UU^T)^2 &=UU^TUU^T \\
            &=U(U^TU)U^T
        \end{split} \nonumber
    \end{equation}
    Because the columns of $U$ is orthonormal, so $U^TU=I_{k\times k}$. Hence
    \begin{equation}
        (UU^T)^2=U(U^TU)U^T=UIU^T=UU^T \nonumber
    \end{equation}
    So $UU^T$ is a projection matrix.
    %%%%%%%%%%%
    % Item (c)
    %%%%%%%%%%%
    \item 
    First we should show $P=A(A^TA)^{-1}A^T$ is a symmetric matrix.
    \begin{equation}
        \begin{split}
            [A(A^TA)^{-1}A^T]^T &=A[(A^TA)^{-1}]^TA^T \\
            &=A[(A^TA)^{T}]^{-1}A^T \\
            &=A(A^TA)^{-1}A^T \\
        \end{split} \nonumber
    \end{equation}
    So $A(A^TA)^{-1}A^T$ is a symmetric matrix. Second we should show that $P=P^2$.
    \begin{equation}
        \begin{split}
            P^2 &=A(A^TA)^{-1}A^TA(A^TA)^{-1}A^T \\
            &=A(A^TA)^{-1}IA^T \\
            &=A(A^TA)^{-1}A^T
        \end{split} \nonumber
    \end{equation}
    Hence $A(A^TA)^{-1}A^T$ is a projection matrix.
    %%%%%%%%%%%
    % Item (d)
    %%%%%%%%%%%
    \item
    Suppose $P=[p_1,p_2,\ldots,p_n]$, where $p_i$ is the column of matrix $P$. Hence $R(P)$ could
    be represented as $span\{p_1,p_2,\ldots,p_n\}$. For any $p_i\in \{p_1,p_2,\ldots,p_n\}$,
    \begin{equation}
        <x-y,p_i>=p_i^T(x-Px)=(p_i^T-p_i^TP)x
    \end{equation}
    If $P$ is a projection matrix, then $P=P^2$ and $P=P^T$must be valid. Therefore,
    \begin{equation}
        P^2=
        \begin{bmatrix}
            p_1^Tp_1 && p_1^Tp_2 && \ldots && p_1^Tp_n \\
            p_2^Tp_1 && p_2^Tp_2 && \ldots && p_2^Tp_n \\
            \vdots   && \vdots   && \      && \vdots   \\
            p_n^Tp_1 && p_n^Tp_2 && \ldots && p_n^Tp_n \\
        \end{bmatrix}=
        \begin{bmatrix}
            <p_1,p_1> && <p_2,p_1> && \ldots && <p_n,p_1> \\
            <p_1,p_2> && <p_2,p_2> && \ldots && <p_n,p_2> \\
            \vdots    && \vdots   &&  \      && \vdots    \\
            <p_1,p_n> && <p_2,p_n> && \ldots && <p_n,p_n> \\
        \end{bmatrix}
        \nonumber
    \end{equation}
    Hence
    \begin{equation}
        p_i=[<p_1,p_1>,<p_1,p_2>,\ldots,<p_1,p_n>]^T \nonumber
    \end{equation}
    Because $p_i\in \mathbb{R}^n$,
    \begin{equation}
        p_i=[<p_1,p_1>,<p_2,p_1>,\ldots,<p_n,p_1>]^T \nonumber
    \end{equation}
    Using the result above into equation (1), then we get
    \begin{equation}
        p_i^T-p_i^TP=p_i^T-[<p_1,p_1>,<p_2,p_1>,\ldots,<p_n,p_1>]=0
        \Rightarrow <x-y,p_i>=0 \nonumber
    \end{equation}
    Hence $y$ is the point in $R(P)$ closest to $x$. $y$ is the projection of x.
    In conclusion, if $P$ is a projection matrix, then $y=Px$ is the projection of x
    onto $R(P)$.
    %%%%%%%%%%%
    % Item (e)
    %%%%%%%%%%%
    \item
    Obviously, the basis of $span{u}$ is $\{u\}$. Hence there is only one solution of the Normal Equation, that is:
    \begin{equation}
        \alpha=(u^Tu)^{-1}u^Tx \nonumber
    \end{equation}
    Therefore
    \begin{equation}
        y=u(u^Tu)^{-1}u^Tx \ \Rightarrow\ P=u(u^Tu)^{-1}u^T=uu^T \nonumber
    \end{equation}
\end{enumerate}

\end{document}
